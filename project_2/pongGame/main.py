import the_agent
import environment
import matplotlib.pyplot as plt
import time
from collections import deque, Counter
import numpy as np
import wandb
import os
from datetime import datetime

# wandb ì´ˆê¸°í™”
wandb.init(
   project="pong-dqn",
   config={
       "game": "ALE/Pong-v5",
       "possible_actions": [0, 2, 3],
       "starting_mem_len": 50000,
       "max_mem_len": 750000,
       "starting_epsilon": 0.1,
       "learn_rate": 0.00025,
       "gamma": 0.95,
       "epsilon_decay": 0.9 / 100000,
       "epsilon_min": 0.05,
       "batch_size": 32,
       "target_update_freq": 10000,
       "save_freq": 50000,
       "episodes": 30
   }
)

name = 'ALE/Pong-v5'

agent = the_agent.Agent(
   possible_actions=[0, 2, 3],
   starting_mem_len=50000,
   max_mem_len=750000,
   starting_epsilon=0.1,
   learn_rate=0.00025
)

# ê¸°ì¡´ ê°€ì¤‘ì¹˜ ë¡œë“œ
if os.path.exists('recent_weights.hdf5'):
   agent.model.load_weights('recent_weights.hdf5')
   agent.model_target.load_weights('recent_weights.hdf5')
   print("ê¸°ì¡´ ê°€ì¤‘ì¹˜ ë¡œë“œ ì™„ë£Œ!")
else:
   print("ê¸°ì¡´ ê°€ì¤‘ì¹˜ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ìƒˆë¡œ í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤.")

env = environment.make_env(name, agent)

# ë¶„ì„ì„ ìœ„í•œ ë³€ìˆ˜ë“¤
last_100_avg = [-21]
scores = deque(maxlen=100)
max_score = -21
total_steps = 0
episode_rewards = []
episode_actions = []
loss_values = []

# ë¡œê·¸ íŒŒì¼ ìƒì„±
log_filename = f"training_log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"
with open(log_filename, 'w', encoding='utf-8') as f:
   f.write("=== PONG DQN í•™ìŠµ ë¡œê·¸ ===\n")
   f.write(f"ì‹œì‘ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
   f.write("=" * 50 + "\n\n")

env.reset()

for i in range(30):
   episode_start_time = time.time()
   timesteps_before = agent.total_timesteps

   # ì—í”¼ì†Œë“œë³„ ë¶„ì„ ë³€ìˆ˜ ì´ˆê¸°í™”
   episode_action_counts = Counter()
   episode_reward_sum = 0
   positive_rewards = 0
   negative_rewards = 0
   exploration_steps = 0

   # ì—í”¼ì†Œë“œ ì‹¤í–‰
   score = environment.play_episode(name, env, agent, debug=False)

   episode_steps = agent.total_timesteps - timesteps_before
   episode_duration = time.time() - episode_start_time
   total_steps += episode_steps

   scores.append(score)
   if score > max_score:
       max_score = score

   avg_score_100 = sum(scores) / len(scores) if len(scores) > 0 else score

   # ë©”ëª¨ë¦¬ì—ì„œ ìµœê·¼ ì—í”¼ì†Œë“œ ë°ì´í„° ë¶„ì„
   if len(agent.memory.actions) > episode_steps:
       recent_actions = list(agent.memory.actions)[-episode_steps:]
       recent_rewards = list(agent.memory.rewards)[-episode_steps:]

       # ì•¡ì…˜ ë¶„í¬ ê³„ì‚°
       episode_action_counts = Counter(recent_actions)

       # ë¦¬ì›Œë“œ ë¶„ì„
       for reward in recent_rewards:
           episode_reward_sum += reward
           if reward > 0:
               positive_rewards += 1
           elif reward < 0:
               negative_rewards += 1

       # íƒí—˜ ë¹„ìœ¨ ê³„ì‚° (epsilon ê¸°ë°˜ ì¶”ì •)
       exploration_steps = int(episode_steps * agent.epsilon)

   # Q-ê°’ ë¶„ì„ (í˜„ì¬ ìƒíƒœ ê¸°ì¤€)
   q_values_mean = 0
   q_values_std = 0
   if len(agent.memory.frames) >= 4:
       state = [agent.memory.frames[-4], agent.memory.frames[-3],
                agent.memory.frames[-2], agent.memory.frames[-1]]
       state = np.moveaxis(state, 0, 2) / 255
       state = np.expand_dims(state, 0)
       q_values = agent.model.predict(state, verbose=0)[0]
       q_values_mean = float(np.mean(q_values))
       q_values_std = float(np.std(q_values))

   # wandb ë¡œê·¸ ê¸°ë¡ (í™•ì¥ëœ ë‚´ìš©)
   log_data = {
       "episode": i,
       "score": score,
       "max_score": max_score,
       "avg_score_100": avg_score_100,
       "episode_steps": episode_steps,
       "total_timesteps": agent.total_timesteps,
       "episode_duration": episode_duration,
       "epsilon": agent.epsilon,
       "learns_count": agent.learns,
       "memory_size": len(agent.memory.frames),

       # ì¶”ê°€ ë¶„ì„ í•­ëª©ë“¤
       "q_value_mean": q_values_mean,
       "q_value_std": q_values_std,
       "positive_rewards": positive_rewards,
       "negative_rewards": negative_rewards,
       "episode_reward_sum": episode_reward_sum,
       "action_0_count": episode_action_counts.get(0, 0),
       "action_2_count": episode_action_counts.get(2, 0),
       "action_3_count": episode_action_counts.get(3, 0),
       "exploration_ratio": exploration_steps / episode_steps if episode_steps > 0 else 0,
       "memory_usage_ratio": len(agent.memory.frames) / agent.memory.max_len,
       "steps_per_second": episode_steps / episode_duration if episode_duration > 0 else 0
   }

   wandb.log(log_data)

   # ì½˜ì†” ì¶œë ¥
   print(f'\n=== Episode: {i} ===')
   print(f'Score: {score} | Max: {max_score} | Avg(100): {avg_score_100:.2f}')
   print(f'Steps: {episode_steps} | Duration: {episode_duration:.2f}s')
   print(f'Epsilon: {agent.epsilon:.4f} | Learns: {agent.learns}')
   print(f'Q-values: Î¼={q_values_mean:.3f}, Ïƒ={q_values_std:.3f}')
   print(
       f'Actions: 0={episode_action_counts.get(0, 0)}, 2={episode_action_counts.get(2, 0)}, 3={episode_action_counts.get(3, 0)}')
   print(f'Rewards: +{positive_rewards}, -{negative_rewards}, Î£={episode_reward_sum}')

   # í…ìŠ¤íŠ¸ íŒŒì¼ì— ë¡œê·¸ ê¸°ë¡
   with open(log_filename, 'a', encoding='utf-8') as f:
       f.write(f"Episode {i:3d} | Score: {score:6.1f} | Steps: {episode_steps:4d} | ")
       f.write(f"Epsilon: {agent.epsilon:.4f} | Q_Î¼: {q_values_mean:6.3f} | ")
       f.write(
           f"Actions(0/2/3): {episode_action_counts.get(0, 0):2d}/{episode_action_counts.get(2, 0):2d}/{episode_action_counts.get(3, 0):2d} | ")
       f.write(f"Rewards(+/-): {positive_rewards:2d}/{negative_rewards:2d}\n")

   # 100 ì—í”¼ì†Œë“œë§ˆë‹¤ ê·¸ë˜í”„ ì €ì¥
   if i % 100 == 0 and i != 0:
       last_100_avg.append(avg_score_100)
       plt.figure(figsize=(10, 6))
       plt.plot(np.arange(0, i + 1, 100), last_100_avg)
       plt.title('Average Score over 100 Episodes')
       plt.xlabel('Episodes')
       plt.ylabel('Average Score')
       plt.grid(True)
       plt.savefig('training_progress.png', dpi=150, bbox_inches='tight')
       wandb.log({"training_progress": wandb.Image('training_progress.png')})
       plt.close()

# í•™ìŠµ ì™„ë£Œ í›„ ëª¨ë¸ ì €ì¥
print("\nğŸ¯ í•™ìŠµ ì™„ë£Œ! ìµœì¢… ëª¨ë¸ ì €ì¥ ì¤‘...")

# íƒ€ì„ìŠ¤íƒ¬í”„ë¡œ ìµœì¢… ëª¨ë¸ ì €ì¥
timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
final_model_name = f'pong_dqn_final_{timestamp}.hdf5'
agent.model.save_weights(final_model_name)
print(f"âœ… ìµœì¢… ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {final_model_name}")

# ìµœê³  ì„±ëŠ¥ ëª¨ë¸ë„ ì €ì¥ (ì„ íƒì‚¬í•­)
best_model_name = f'pong_dqn_best_{max_score}pts_{timestamp}.hdf5'
agent.model.save_weights(best_model_name)
print(f"âœ… ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {best_model_name}")

# ìµœì¢… ìš”ì•½ í†µê³„
final_stats = {
   "final_max_score": max_score,
   "final_avg_score": avg_score_100,
   "total_episodes": i + 1,
   "total_timesteps": agent.total_timesteps,
   "total_learns": agent.learns,
   "final_epsilon": agent.epsilon,
   "final_memory_size": len(agent.memory.frames),
   "final_model_saved": True,
   "final_model_name": final_model_name
}

wandb.log(final_stats)

# í…ìŠ¤íŠ¸ íŒŒì¼ì— ìµœì¢… ìš”ì•½ ê¸°ë¡
with open(log_filename, 'a', encoding='utf-8') as f:
   f.write("\n" + "=" * 50 + "\n")
   f.write("=== ìµœì¢… í•™ìŠµ ê²°ê³¼ ìš”ì•½ ===\n")
   f.write(f"ì¢…ë£Œ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
   f.write(f"ì´ ì—í”¼ì†Œë“œ: {i + 1}\n")
   f.write(f"ìµœê³  ì ìˆ˜: {max_score}\n")
   f.write(f"í‰ê·  ì ìˆ˜ (ìµœê·¼ 100): {avg_score_100:.2f}\n")
   f.write(f"ì´ í•™ìŠµ ìŠ¤í…: {agent.total_timesteps}\n")
   f.write(f"ì´ í•™ìŠµ íšŸìˆ˜: {agent.learns}\n")
   f.write(f"ìµœì¢… Epsilon: {agent.epsilon:.4f}\n")
   f.write(f"ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {len(agent.memory.frames)} / {agent.memory.max_len}\n")
   f.write(f"ìµœì¢… ëª¨ë¸: {final_model_name}\n")
   f.write("=" * 50 + "\n")

print(f"\nâœ… í•™ìŠµ ì™„ë£Œ! ë¡œê·¸ íŒŒì¼ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {log_filename}")

wandb.finish()