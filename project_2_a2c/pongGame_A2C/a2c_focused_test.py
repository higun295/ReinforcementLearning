import sys

sys.path.append("../src/")
import wandb
import os
import time
import matplotlib.pyplot as plt
from config import *
from pong_wrapper import *
from process_image import *
from utilities import *
from network import *
from agent import *

# A2C ì§‘ì¤‘ ìµœì í™” ì¡°í•©ë“¤ (8ê°œ)
A2C_FOCUSED_COMBINATIONS = [
    # Phase 1: ê¸°ë³¸ A2C ìµœì í™” (4ê°œ)
    {
        "name": "a2c_baseline_optimized",
        "agent": "A2C",
        "lr": 7e-4,
        "value_c": 1.0,
        "entropy_c": 0.015,
        "batch_size": 128,
        "n_updates": 3000
    },
    {
        "name": "a2c_higher_lr",
        "agent": "A2C",
        "lr": 1.5e-3,
        "value_c": 1.5,
        "entropy_c": 0.02,
        "batch_size": 128,
        "n_updates": 3000
    },
    {
        "name": "a2c_bigger_batch",
        "agent": "A2C",
        "lr": 1e-3,
        "value_c": 2.0,
        "entropy_c": 0.015,
        "batch_size": 256,
        "n_updates": 3000
    },
    {
        "name": "a2c_high_exploration",
        "agent": "A2C",
        "lr": 8e-4,
        "value_c": 1.0,
        "entropy_c": 0.03,
        "batch_size": 128,
        "n_updates": 3000
    },

    # Phase 2: ë” aggressiveí•œ A2C ì„¤ì • (4ê°œ)
    {
        "name": "a2c_very_high_lr",
        "agent": "A2C",
        "lr": 2e-3,
        "value_c": 2.0,
        "entropy_c": 0.01,
        "batch_size": 192,
        "n_updates": 4000
    },
    {
        "name": "a2c_balanced_strong",
        "agent": "A2C",
        "lr": 1.2e-3,
        "value_c": 3.0,
        "entropy_c": 0.025,
        "batch_size": 256,
        "n_updates": 4000
    },
    {
        "name": "a2c_conservative_stable",
        "agent": "A2C",
        "lr": 5e-4,
        "value_c": 0.5,
        "entropy_c": 0.01,
        "batch_size": 128,
        "n_updates": 4000
    },
    {
        "name": "a2c_final_candidate",
        "agent": "A2C",
        "lr": 1e-3,
        "value_c": 1.5,
        "entropy_c": 0.018,
        "batch_size": 192,
        "n_updates": 5000  # ê°€ì¥ ê¸´ í•™ìŠµ
    }
]


def run_a2c_test(params):
    """A2C ì „ìš© í…ŒìŠ¤íŠ¸"""
    print(f"\n{'=' * 60}")
    print(f"ğŸ”¥ A2C í…ŒìŠ¤íŠ¸: {params['name']}")
    print(f"LR: {params['lr']}, VALUE_C: {params['value_c']}")
    print(f"ENTROPY_C: {params['entropy_c']}, BATCH: {params['batch_size']}")
    print(f"UPDATES: {params['n_updates']}")
    print(f"{'=' * 60}")

    try:
        print("1. A2C ì„¤ì • ì¤€ë¹„ ì¤‘...")

        # A2C ì „ìš© config
        test_config = {
            "agent": "A2C",
            "input_shape": INPUT_SHAPE,
            "batch_size": params['batch_size'],
            "n_updates": params['n_updates'],
            "hidden": HIDDEN,
            "learning_rate": params['lr'],
            "gamma": GAMMA,
            "value_c": params['value_c'],
            "entropy_c": params['entropy_c'],
            "std_adv": STD_ADV,
            "test_name": params['name'],
            "operating_system": os.name
        }

        print("2. WandB ì´ˆê¸°í™” ì¤‘...")
        run_name = f"A2C_{params['name']}_lr{params['lr']}_vc{params['value_c']}_ent{params['entropy_c']}"

        wandb.init(
            project="tensorflow2_pong_a2c_focused",
            name=run_name,
            tags=["A2C", "CNN", "RL", "atari_pong", "focused_test", params['name']],
            config=test_config,
            mode="online"
        )

        print(f"WandB URL: {wandb.run.url}")

        print("3. í™˜ê²½ ë° ëª¨ë¸ ì´ˆê¸°í™” ì¤‘...")
        pw = PongWrapper(ENV_NAME, history_length=4)
        model = Model(num_actions=pw.env.action_space.n, hidden=HIDDEN)

        print("4. A2C ì—ì´ì „íŠ¸ ì´ˆê¸°í™” ì¤‘...")
        agent = Agent(
            model=model,
            save_path=f"../model/a2c_focused_{params['name']}",
            load_path=None,
            lr=params['lr'],
            gamma=GAMMA,
            value_c=params['value_c'],
            entropy_c=params['entropy_c'],
            clip_ratio=0.2,  # A2Cì—ì„œëŠ” ì‚¬ìš© ì•ˆë˜ì§€ë§Œ ê¸°ë³¸ê°’
            std_adv=STD_ADV,
            agent="A2C",
            input_shape=INPUT_SHAPE,
            batch_size=params['batch_size'],
            updates=params['n_updates']
        )

        print("5. A2C í•™ìŠµ ì‹œì‘...")
        start_time = time.time()
        rewards_history = agent.train(pw)
        end_time = time.time()
        print("A2C í•™ìŠµ ì™„ë£Œ!")

        # ìƒì„¸ ë¶„ì„
        total_time = end_time - start_time

        # ë‹¨ê³„ë³„ ì„±ëŠ¥ ë¶„ì„
        episodes_count = len(rewards_history)
        if episodes_count >= 200:
            first_50 = np.mean(rewards_history[:50])
            second_50 = np.mean(rewards_history[50:100]) if episodes_count >= 100 else first_50
            third_50 = np.mean(rewards_history[100:150]) if episodes_count >= 150 else second_50
            last_50 = np.mean(rewards_history[-50:])
        else:
            quarter = episodes_count // 4
            first_50 = np.mean(rewards_history[:quarter]) if quarter > 0 else -21
            second_50 = np.mean(rewards_history[quarter:quarter * 2]) if quarter > 0 else first_50
            third_50 = np.mean(rewards_history[quarter * 2:quarter * 3]) if quarter > 0 else second_50
            last_50 = np.mean(rewards_history[quarter * 3:]) if quarter > 0 else third_50

        max_reward = max(rewards_history) if rewards_history else -21
        min_reward = min(rewards_history) if rewards_history else -21
        overall_improvement = last_50 - first_50

        # í•™ìŠµ ì•ˆì •ì„± ì ìˆ˜
        if episodes_count >= 100:
            recent_std = np.std(rewards_history[-100:])
            stability_score = max(0, 5 - recent_std)  # í‘œì¤€í¸ì°¨ê°€ ë‚®ì„ìˆ˜ë¡ ë†’ì€ ì ìˆ˜
        else:
            stability_score = 0

        # ì¢…í•© ì ìˆ˜ ê³„ì‚°
        final_score = (
                (max_reward + 21) * 2 +  # ìµœê³  ì„±ëŠ¥ (0~42ì )
                (overall_improvement + 5) * 3 +  # ê°œì„ ë„ (-5~+5 â†’ 0~30ì )
                stability_score * 2  # ì•ˆì •ì„± (0~10ì )
        )

        print(f"\nğŸ“Š A2C ì„±ëŠ¥ ë¶„ì„:")
        print(f"    ì²« 50 ì—í”¼ì†Œë“œ í‰ê· : {first_50:.2f}")
        print(f"    ë§ˆì§€ë§‰ 50 ì—í”¼ì†Œë“œ í‰ê· : {last_50:.2f}")
        print(f"    ì „ì²´ ê°œì„ ë„: {overall_improvement:.2f}")
        print(f"    ìµœê³  ë¦¬ì›Œë“œ: {max_reward:.2f}")
        print(f"    ì•ˆì •ì„± ì ìˆ˜: {stability_score:.2f}")
        print(f"    ì¢…í•© ì ìˆ˜: {final_score:.2f}")

        # ê²°ê³¼ ì €ì¥
        timestamp = time.strftime('%Y%m%d%H%M')
        save_dir = f"../model/a2c_focused_{params['name']}/save_agent_{timestamp}"
        plot_dir = os.path.join(save_dir, "plots")
        os.makedirs(plot_dir, exist_ok=True)

        # ìƒì„¸ ê·¸ë˜í”„ ìƒì„±
        plt.figure(figsize=(15, 10))

        # ì „ì²´ ë¦¬ì›Œë“œ ê·¸ë˜í”„
        plt.subplot(2, 3, 1)
        plt.plot(rewards_history, alpha=0.7)
        plt.xlabel("Episodes")
        plt.ylabel("Reward")
        plt.title(f"Episode Rewards - {params['name']}")
        plt.grid(True)

        # Moving average ê·¸ë˜í”„
        plt.subplot(2, 3, 2)
        if len(rewards_history) > 20:
            window = min(50, len(rewards_history) // 4)
            moving_avg = []
            for i in range(window, len(rewards_history)):
                moving_avg.append(np.mean(rewards_history[i - window:i]))
            plt.plot(moving_avg)
            plt.xlabel("Episodes")
            plt.ylabel(f"{window}-Episode Moving Average")
            plt.title("Moving Average Rewards")
            plt.grid(True)

        # ë‹¨ê³„ë³„ ì„±ëŠ¥ ë¹„êµ
        plt.subplot(2, 3, 3)
        stages = ['First 50', 'Second 50', 'Third 50', 'Last 50']
        values = [first_50, second_50, third_50, last_50]
        plt.bar(stages, values, color=['red', 'orange', 'yellow', 'green'])
        plt.ylabel("Average Reward")
        plt.title("Performance by Stage")
        plt.xticks(rotation=45)

        # ìµœê·¼ 100 ì—í”¼ì†Œë“œ ë¶„í¬
        plt.subplot(2, 3, 4)
        recent_rewards = rewards_history[-100:] if len(rewards_history) >= 100 else rewards_history
        plt.hist(recent_rewards, bins=20, alpha=0.7)
        plt.xlabel("Reward")
        plt.ylabel("Frequency")
        plt.title("Recent Reward Distribution")

        # ê°œì„  ì¶”ì„¸
        plt.subplot(2, 3, 5)
        if len(rewards_history) > 10:
            chunk_size = max(10, len(rewards_history) // 20)
            chunk_means = []
            for i in range(0, len(rewards_history), chunk_size):
                chunk = rewards_history[i:i + chunk_size]
                chunk_means.append(np.mean(chunk))
            plt.plot(chunk_means, marker='o')
            plt.xlabel(f"Chunks of {chunk_size} episodes")
            plt.ylabel("Average Reward")
            plt.title("Learning Progress")
            plt.grid(True)

        # ì¢…í•© ì ìˆ˜ í‘œì‹œ
        plt.subplot(2, 3, 6)
        score_components = ['Max Reward', 'Improvement', 'Stability', 'Total']
        score_values = [
            (max_reward + 21) * 2,
            (overall_improvement + 5) * 3,
            stability_score * 2,
            final_score
        ]
        colors = ['blue', 'green', 'orange', 'red']
        plt.bar(score_components, score_values, color=colors)
        plt.ylabel("Score")
        plt.title("Performance Score Breakdown")
        plt.xticks(rotation=45)

        plt.tight_layout()
        plot_path = os.path.join(plot_dir, "a2c_detailed_analysis.png")
        plt.savefig(plot_path, dpi=150, bbox_inches='tight')
        plt.close()

        # ëª¨ë¸ ì €ì¥ ë° í…ŒìŠ¤íŠ¸
        model_path = f"{save_dir}/model.tf"
        video_path = f"{save_dir}/test_play.mp4"

        agent.save_model(model_path)
        final_test_reward = agent.test(pw, render=False, record_path=video_path)

        # WandBì— ìƒì„¸ ê²°ê³¼ ë¡œê·¸
        wandb.log({
            "final_test_reward": final_test_reward,
            "max_reward_achieved": max_reward,
            "min_reward_achieved": min_reward,
            "overall_improvement": overall_improvement,
            "first_50_avg": first_50,
            "last_50_avg": last_50,
            "stability_score": stability_score,
            "final_score": final_score,
            "total_training_time_minutes": total_time / 60,
            "total_episodes": episodes_count
        })

        print(f"[âœ”] {params['name']} ì™„ë£Œ! ì¢…í•© ì ìˆ˜: {final_score:.2f}")
        print(f"    ëª¨ë¸: {model_path}")
        print(f"    ì˜ìƒ: {video_path}")

        return {
            "name": params['name'],
            "final_score": final_score,
            "max_reward": max_reward,
            "improvement": overall_improvement,
            "stability": stability_score,
            "final_test_reward": final_test_reward,
            "time_minutes": total_time / 60,
            "episodes": episodes_count,
            "params": params
        }

    except Exception as e:
        print(f"[âœ–] {params['name']} ì˜¤ë¥˜: {e}")
        return None

    finally:
        wandb.finish()


def main():
    """A2C ì§‘ì¤‘ í…ŒìŠ¤íŠ¸ ë©”ì¸"""
    print("ğŸ¯ A2C ì§‘ì¤‘ ìµœì í™” í…ŒìŠ¤íŠ¸ ì‹œì‘!")
    print(f"ì´ {len(A2C_FOCUSED_COMBINATIONS)}ê°œ A2C ì¡°í•© í…ŒìŠ¤íŠ¸")
    print("PPOëŠ” í¬ê¸°í•˜ê³  A2Cë¡œ ëŒíŒŒêµ¬ë¥¼ ì°¾ì•„ë³´ì! ğŸ’ª")

    results = []
    start_total_time = time.time()

    for i, params in enumerate(A2C_FOCUSED_COMBINATIONS, 1):
        print(f"\nğŸ”„ A2C í…ŒìŠ¤íŠ¸ ì§„í–‰ë¥ : {i}/{len(A2C_FOCUSED_COMBINATIONS)}")
        estimated_time = (len(A2C_FOCUSED_COMBINATIONS) - i) * 35  # A2CëŠ” ì•½ 35ë¶„ ì¶”ì •
        print(f"â° ì˜ˆìƒ ë‚¨ì€ ì‹œê°„: {estimated_time // 60}ì‹œê°„ {estimated_time % 60}ë¶„")

        result = run_a2c_test(params)
        if result:
            results.append(result)

        # ë©”ëª¨ë¦¬ ì •ë¦¬
        if i < len(A2C_FOCUSED_COMBINATIONS):
            print("ë‹¤ìŒ A2C í…ŒìŠ¤íŠ¸ ì¤€ë¹„ ì¤‘... (10ì´ˆ ëŒ€ê¸°)")
            time.sleep(10)

    # ìµœì¢… A2C ê²°ê³¼ ë¶„ì„
    total_time = time.time() - start_total_time
    print(f"\n{'=' * 80}")
    print("ğŸ† A2C ì§‘ì¤‘ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
    print(f"ì´ ì†Œìš” ì‹œê°„: {total_time / 3600:.1f}ì‹œê°„")
    print(f"\nğŸ“Š A2C ì„±ëŠ¥ ìˆœìœ„ (ì¢…í•© ì ìˆ˜ ê¸°ì¤€):")
    print("-" * 80)

    if results:
        results.sort(key=lambda x: x['final_score'], reverse=True)

        print(f"{'ìˆœìœ„':<4} {'ì´ë¦„':<25} {'ì¢…í•©ì ìˆ˜':<8} {'ìµœê³ ë¦¬ì›Œë“œ':<8} {'ê°œì„ ë„':<8} {'ì•ˆì •ì„±':<8}")
        print("-" * 80)

        for i, result in enumerate(results, 1):
            print(
                f"{i:<4} {result['name']:<25} {result['final_score']:<8.1f} {result['max_reward']:<8.1f} {result['improvement']:<8.2f} {result['stability']:<8.1f}")

        print(f"\nğŸ¥‡ ìµœê³  A2C ì¡°í•©:")
        best = results[0]
        print(f"    ì´ë¦„: {best['name']}")
        print(f"    ì¢…í•© ì ìˆ˜: {best['final_score']:.1f}")
        print(f"    ìµœê³  ë¦¬ì›Œë“œ: {best['max_reward']:.1f}")
        print(f"    ê°œì„ ë„: {best['improvement']:.2f}")
        print(f"\nğŸ”§ ìµœì  A2C íŒŒë¼ë¯¸í„°:")
        for key, value in best['params'].items():
            if key != 'name':
                print(f"    {key}: {value}")

        print(f"\nğŸš€ ë‹¤ìŒ ë‹¨ê³„:")
        print(f"    1. ìµœê³  ì„±ëŠ¥ A2C ì¡°í•©ìœ¼ë¡œ 10000+ ì—…ë°ì´íŠ¸ ì¥ê¸° í•™ìŠµ")
        print(f"    2. ìƒìœ„ 3ê°œ ì¡°í•© ì„¸ë¶€ íŠœë‹")
        print(f"    3. ë„¤íŠ¸ì›Œí¬ êµ¬ì¡° ê°œì„  ì‹œë„")

    else:
        print("âŒ ì„±ê³µí•œ A2C í…ŒìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")

    print(f"{'=' * 80}")


if __name__ == "__main__":
    main()