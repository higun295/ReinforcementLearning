import sys

sys.path.append("../src/")
import wandb
import os
import time
import matplotlib.pyplot as plt
from config import *
from pong_wrapper import *
from process_image import *
from utilities import *
from network import *
from agent import *

# 5-6ì‹œê°„ í™•ì¥ í…ŒìŠ¤íŠ¸ìš© ì¡°í•©ë“¤ (ì´ 12ê°œ)
EXTENDED_TEST_COMBINATIONS = [
    # Phase 1: Value Function ë¬¸ì œ í•´ê²° ì§‘ì¤‘ (4ê°œ)
    {
        "name": "emergency_high_lr",
        "lr": 2e-3,
        "value_c": 3.0,
        "clip_ratio": 0.3,
        "entropy_c": 0.01,
        "batch_size": 128,
        "agent": "PPO"
    },
    {
        "name": "extreme_value_focus",
        "lr": 1e-3,
        "value_c": 10.0,
        "clip_ratio": 0.2,
        "entropy_c": 0.005,
        "batch_size": 256,
        "agent": "PPO"
    },
    {
        "name": "try_a2c_high_lr",
        "lr": 1e-3,
        "value_c": 2.0,
        "entropy_c": 0.01,
        "batch_size": 128,
        "agent": "A2C"
    },
    {
        "name": "try_a2c_baseline",
        "lr": 7e-4,
        "value_c": 1.0,
        "entropy_c": 0.015,
        "batch_size": 128,
        "agent": "A2C"
    },

    # Phase 2: ê·¹ë‹¨ì  íŒŒë¼ë¯¸í„° ì‹¤í—˜ (4ê°œ)
    {
        "name": "very_high_lr",
        "lr": 5e-3,
        "value_c": 2.0,
        "clip_ratio": 0.4,
        "entropy_c": 0.005,
        "batch_size": 64,
        "agent": "PPO"
    },
    {
        "name": "huge_batch",
        "lr": 1e-3,
        "value_c": 2.0,
        "clip_ratio": 0.2,
        "entropy_c": 0.01,
        "batch_size": 512,
        "agent": "PPO"
    },
    {
        "name": "high_exploration_fixed",
        "lr": 1e-3,
        "value_c": 3.0,
        "clip_ratio": 0.2,
        "entropy_c": 0.05,
        "batch_size": 128,
        "agent": "PPO"
    },
    {
        "name": "conservative_but_stable",
        "lr": 5e-4,
        "value_c": 5.0,
        "clip_ratio": 0.1,
        "entropy_c": 0.01,
        "batch_size": 256,
        "agent": "PPO"
    },

    # Phase 3: ìœ ë§í•œ ì¡°í•© longer run (4ê°œ - ë” ê¸´ í•™ìŠµ)
    {
        "name": "promising_long_1",
        "lr": 1e-3,
        "value_c": 3.0,
        "clip_ratio": 0.2,
        "entropy_c": 0.01,
        "batch_size": 128,
        "agent": "PPO",
        "n_updates": 3000  # ë” ê¸´ í•™ìŠµ
    },
    {
        "name": "promising_long_2",
        "lr": 2e-3,
        "value_c": 2.0,
        "clip_ratio": 0.3,
        "entropy_c": 0.005,
        "batch_size": 256,
        "agent": "PPO",
        "n_updates": 3000
    },
    {
        "name": "a2c_long_test",
        "lr": 1e-3,
        "value_c": 2.0,
        "entropy_c": 0.01,
        "batch_size": 128,
        "agent": "A2C",
        "n_updates": 3000
    },
    {
        "name": "final_best_guess",
        "lr": 1.5e-3,
        "value_c": 4.0,
        "clip_ratio": 0.25,
        "entropy_c": 0.008,
        "batch_size": 192,
        "agent": "PPO",
        "n_updates": 3000
    }
]

# ê¸°ë³¸ í…ŒìŠ¤íŠ¸ ì„¤ì •
DEFAULT_BATCH_SIZE = 128
DEFAULT_N_UPDATES = 2000


def run_single_test(params):
    """ë‹¨ì¼ íŒŒë¼ë¯¸í„° ì¡°í•© í…ŒìŠ¤íŠ¸"""
    print(f"\n{'=' * 60}")
    print(f"í…ŒìŠ¤íŠ¸ ì‹œì‘: {params['name']}")
    print(f"Agent: {params.get('agent', 'PPO')}")
    print(f"LR: {params['lr']}, VALUE_C: {params.get('value_c', 0.5)}")
    print(f"CLIP: {params.get('clip_ratio', 0.2)}, ENTROPY: {params['entropy_c']}")
    print(f"BATCH: {params.get('batch_size', DEFAULT_BATCH_SIZE)}")
    print(f"UPDATES: {params.get('n_updates', DEFAULT_N_UPDATES)}")
    print(f"{'=' * 60}")

    try:
        print("1. ì„¤ì • ì¤€ë¹„ ì¤‘...")

        # íŒŒë¼ë¯¸í„° ê¸°ë³¸ê°’ ì„¤ì •
        test_lr = params['lr']
        test_value_c = params.get('value_c', VALUE_C)
        test_clip_ratio = params.get('clip_ratio', CLIP_RATIO)
        test_entropy_c = params['entropy_c']
        test_batch_size = params.get('batch_size', DEFAULT_BATCH_SIZE)
        test_n_updates = params.get('n_updates', DEFAULT_N_UPDATES)
        test_agent = params.get('agent', AGENT)

        # í˜„ì¬ í…ŒìŠ¤íŠ¸ìš© config ìƒì„±
        test_config = CONFIG_WANDB.copy()
        test_config.update({
            "batch_size": test_batch_size,
            "n_updates": test_n_updates,
            "learning_rate": test_lr,
            "value_c": test_value_c,
            "entropy_c": test_entropy_c,
            "clip_ratio": test_clip_ratio,
            "agent": test_agent,
            "test_name": params['name']
        })

        print("2. WandB ì´ˆê¸°í™” ì¤‘...")
        # WandB ì´ˆê¸°í™”
        run_name = f"{test_agent}_{params['name']}_lr{test_lr}_vc{test_value_c}"

        print(f"   í”„ë¡œì íŠ¸ëª…: tensorflow2_pong_extended_test")
        print(f"   ì‹¤í–‰ëª…: {run_name}")

        wandb.init(
            project="tensorflow2_pong_extended_test",
            name=run_name,
            tags=["extended_test", test_agent, "CNN", "RL", "atari_pong", params['name']],
            config=test_config,
            mode="online"
        )

        print(f"WandB URL: {wandb.run.url}")

        print("3. í™˜ê²½ ì´ˆê¸°í™” ì¤‘...")
        pw = PongWrapper(ENV_NAME, history_length=4)
        print("4. ëª¨ë¸ ì´ˆê¸°í™” ì¤‘...")
        model = Model(num_actions=pw.env.action_space.n, hidden=HIDDEN)
        print("5. ì—ì´ì „íŠ¸ ì´ˆê¸°í™” ì¤‘...")

        # ì—ì´ì „íŠ¸ ì´ˆê¸°í™”
        agent = Agent(
            model=model,
            save_path=f"../model/extended_test_{params['name']}",
            load_path=None,
            lr=test_lr,
            gamma=GAMMA,
            value_c=test_value_c,
            entropy_c=test_entropy_c,
            clip_ratio=test_clip_ratio,
            std_adv=STD_ADV,
            agent=test_agent,
            input_shape=INPUT_SHAPE,
            batch_size=test_batch_size,
            updates=test_n_updates
        )

        print("6. í•™ìŠµ ì‹œì‘...")
        start_time = time.time()
        rewards_history = agent.train(pw)
        end_time = time.time()
        print("í•™ìŠµ ì™„ë£Œ!")

        # ê²°ê³¼ ì €ì¥
        timestamp = time.strftime('%Y%m%d%H%M')
        save_dir = f"../model/extended_test_{params['name']}/save_agent_{timestamp}"
        plot_dir = os.path.join(save_dir, "plots")
        os.makedirs(plot_dir, exist_ok=True)

        # ê·¸ë˜í”„ ì €ì¥
        plt.figure(figsize=(12, 8))
        plt.subplot(2, 1, 1)
        plt.plot(rewards_history)
        plt.xlabel("Episodes")
        plt.ylabel("Reward")
        plt.title(f"Episode Rewards - {params['name']}")
        plt.grid(True)

        plt.subplot(2, 1, 2)
        if len(rewards_history) > 50:
            moving_avg = []
            window = 50
            for i in range(window, len(rewards_history)):
                moving_avg.append(np.mean(rewards_history[i - window:i]))
            plt.plot(moving_avg)
            plt.xlabel("Episodes")
            plt.ylabel("50-Episode Moving Average")
            plt.title(f"Moving Average Rewards - {params['name']}")
            plt.grid(True)

        plt.tight_layout()
        plot_path = os.path.join(plot_dir, "detailed_reward_plot.png")
        plt.savefig(plot_path)
        plt.close()

        # ëª¨ë¸ ì €ì¥ ë° í…ŒìŠ¤íŠ¸
        model_path = f"{save_dir}/model.tf"
        video_path = f"{save_dir}/test_play.mp4"

        agent.save_model(model_path)
        final_reward = agent.test(pw, render=False, record_path=video_path)

        # ê²°ê³¼ ë¶„ì„
        total_time = end_time - start_time
        avg_reward_last_100 = np.mean(rewards_history[-100:]) if len(rewards_history) >= 100 else np.mean(
            rewards_history)
        avg_reward_last_50 = np.mean(rewards_history[-50:]) if len(rewards_history) >= 50 else np.mean(rewards_history)
        max_reward = max(rewards_history) if rewards_history else -21
        min_reward = min(rewards_history) if rewards_history else -21

        # ê°œì„ ë„ ê³„ì‚°
        first_100 = np.mean(rewards_history[:100]) if len(rewards_history) >= 100 else np.mean(
            rewards_history[:len(rewards_history) // 2])
        improvement = avg_reward_last_100 - first_100

        # ìµœì¢… ê²°ê³¼ ë¡œê·¸
        wandb.log({
            "final_test_reward": final_reward,
            "avg_reward_last_100_episodes": avg_reward_last_100,
            "avg_reward_last_50_episodes": avg_reward_last_50,
            "max_reward_achieved": max_reward,
            "min_reward_achieved": min_reward,
            "improvement_score": improvement,
            "total_training_time_minutes": total_time / 60,
            "total_episodes": len(rewards_history)
        })

        print(f"\n[âœ”] {params['name']} í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
        print(f"    ìµœì¢… í…ŒìŠ¤íŠ¸ ë¦¬ì›Œë“œ: {final_reward:.2f}")
        print(f"    ìµœê·¼ 100 ì—í”¼ì†Œë“œ í‰ê· : {avg_reward_last_100:.2f}")
        print(f"    ìµœê³  ë¦¬ì›Œë“œ: {max_reward:.2f}")
        print(f"    ê°œì„ ë„: {improvement:.2f}")
        print(f"    ì†Œìš” ì‹œê°„: {total_time / 60:.1f}ë¶„")

        return {
            "name": params['name'],
            "agent": test_agent,
            "final_reward": final_reward,
            "avg_reward_100": avg_reward_last_100,
            "avg_reward_50": avg_reward_last_50,
            "max_reward": max_reward,
            "improvement": improvement,
            "time_minutes": total_time / 60,
            "episodes": len(rewards_history),
            "params": params
        }

    except Exception as e:
        print(f"[âœ–] {params['name']} í…ŒìŠ¤íŠ¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None

    finally:
        wandb.finish()


def main():
    """ë©”ì¸ í•¨ìˆ˜ - í™•ì¥ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    print("ğŸš€ 5-6ì‹œê°„ í™•ì¥ í•˜ì´í¼íŒŒë¼ë¯¸í„° í…ŒìŠ¤íŠ¸ ì‹œì‘!")
    print(f"ì´ {len(EXTENDED_TEST_COMBINATIONS)}ê°œ ì¡°í•© í…ŒìŠ¤íŠ¸ ì˜ˆì •")
    print(f"ì˜ˆìƒ ì†Œìš” ì‹œê°„: 5-6ì‹œê°„")

    results = []
    start_total_time = time.time()

    for i, params in enumerate(EXTENDED_TEST_COMBINATIONS, 1):
        print(f"\nğŸ”„ ì§„í–‰ë¥ : {i}/{len(EXTENDED_TEST_COMBINATIONS)} ({i / len(EXTENDED_TEST_COMBINATIONS) * 100:.1f}%)")
        estimated_remaining = (len(EXTENDED_TEST_COMBINATIONS) - i) * 25  # í‰ê·  25ë¶„ ì¶”ì •
        print(f"â° ì˜ˆìƒ ë‚¨ì€ ì‹œê°„: {estimated_remaining // 60}ì‹œê°„ {estimated_remaining % 60}ë¶„")

        result = run_single_test(params)
        if result:
            results.append(result)

        # ë‹¤ìŒ í…ŒìŠ¤íŠ¸ ì „ ë©”ëª¨ë¦¬ ì •ë¦¬
        if i < len(EXTENDED_TEST_COMBINATIONS):
            print("ë‹¤ìŒ í…ŒìŠ¤íŠ¸ ì¤€ë¹„ ì¤‘... (10ì´ˆ ëŒ€ê¸°)")
            time.sleep(10)

    # ì „ì²´ ê²°ê³¼ ìš”ì•½
    total_time = time.time() - start_total_time
    print(f"\n{'=' * 80}")
    print("ğŸ‰ ëª¨ë“  í™•ì¥ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
    print(f"ì´ ì†Œìš” ì‹œê°„: {total_time / 3600:.1f}ì‹œê°„")
    print(f"\nğŸ“Š ê²°ê³¼ ìš”ì•½ (ê°œì„ ë„ ê¸°ì¤€ ì •ë ¬):")
    print("-" * 80)

    if results:
        # ê²°ê³¼ ì •ë ¬ (ê°œì„ ë„ ê¸°ì¤€)
        results.sort(key=lambda x: x['improvement'], reverse=True)

        print(f"{'ìˆœìœ„':<4} {'ì´ë¦„':<20} {'Agent':<5} {'ê°œì„ ë„':<8} {'ìµœì¢…ë¦¬ì›Œë“œ':<8} {'ìµœê³ ë¦¬ì›Œë“œ':<8}")
        print("-" * 80)

        for i, result in enumerate(results[:10], 1):  # ìƒìœ„ 10ê°œë§Œ í‘œì‹œ
            print(
                f"{i:<4} {result['name']:<20} {result['agent']:<5} {result['improvement']:<8.2f} {result['final_reward']:<8.2f} {result['max_reward']:<8.2f}")

        print(f"\nğŸ† ìµœê³  ì„±ëŠ¥ ì¡°í•©:")
        best_config = results[0]
        print(f"    ì´ë¦„: {best_config['name']}")
        print(f"    Agent: {best_config['agent']}")
        print(f"    ê°œì„ ë„: {best_config['improvement']:.2f}")
        print(f"    ìµœì¢… ë¦¬ì›Œë“œ: {best_config['final_reward']:.2f}")
        print(f"    íŒŒë¼ë¯¸í„°:")
        for key, value in best_config['params'].items():
            if key != 'name':
                print(f"      {key}: {value}")

        print(f"\nğŸ’¡ ì¶”ì²œ ë‹¤ìŒ ë‹¨ê³„:")
        print(f"    1. ìƒìœ„ 3ê°œ ì¡°í•©ì„ ë” ê¸´ í•™ìŠµ(10000+ ì—…ë°ì´íŠ¸)ìœ¼ë¡œ ì¬í…ŒìŠ¤íŠ¸")
        print(f"    2. ê°€ì¥ ì¢‹ì€ ì¡°í•© ì£¼ë³€ íŒŒë¼ë¯¸í„° fine-tuning")
        print(f"    3. ë„¤íŠ¸ì›Œí¬ êµ¬ì¡° ê°œì„  ê³ ë ¤")

    else:
        print("âŒ ì„±ê³µí•œ í…ŒìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")

    print(f"{'=' * 80}")


if __name__ == "__main__":
    main()